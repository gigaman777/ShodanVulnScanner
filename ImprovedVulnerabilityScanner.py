from bs4 import BeautifulSoup
from shodan import Shodan
import time
import requests
import smtplib
import csv

# function to start soup and find the data
def find_result(name):
    # url and get page
    URL = "https://www.cisa.gov/known-exploited-vulnerabilities-catalog"
    page = requests.get(URL)

    # create soup
    soup = BeautifulSoup(page.content, "html.parser")

    # get the results for the page
    results = soup.find_all("td", class_=name)

    # return the results formatted
    return format_results(results)


# function to strip tags and extra info from the results
def format_results(result):
    return result[len(result)-1].text


# function to search shodan using count and return the amout of vulnerable websites
def search_shodan(product):
    # shodan login with api key
    SHODAN_API_KEY = 'SwTWccfpRgnooEeM8FTf7JZVmv7wMzoV'

    api = Shodan(SHODAN_API_KEY)

    # return count of vulnerable applications
    return api.count(product)


# key used later to determine if the code is the same
cve_key = ""

while True:
    # formatted with only one result and the text
    formatted_cve_result = find_result("cve")
    formatted_name_result = find_result("vuln-name")
    formatted_product_result = find_result("product")

    # check if the output needs to be ignored because it is the same as the last result
    if formatted_cve_result != cve_key:
        print("cve: " + formatted_cve_result + "\tname: " +
              formatted_name_result + "\tproduct: " + formatted_product_result)

        # use shodan api to get all vulnerable websites and print
        vulnerable_sites_amount = search_shodan(formatted_product_result)
        print(vulnerable_sites_amount)

        # save results to csv
        # need to reformat
        with open('sites.csv', 'w', newline='') as file:
            writer = csv.writer(file)
            writer.writerows(vulnerable_sites_amount)

    cve_key = formatted_cve_result

    time.sleep(3)
     